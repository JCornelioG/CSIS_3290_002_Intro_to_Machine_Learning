{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d992d0",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Scikit-Learn\n",
    "\n",
    "This unit focuses on both supervised and unsupervised machine learning techniques. Our primary tool will be `scikit-learn`, a comprehensive and widely-adopted machine learning library in Python that has become an industry standard.\n",
    "\n",
    "## What is Scikit-Learn?\n",
    "\n",
    "`scikit-learn` offers a vast array of machine learning tools and utilities. These include:\n",
    "\n",
    "- **Data Preprocessing:** Tools for cleaning, normalizing, and preparing data for analysis.\n",
    "- **Statistical Models:** Implementations of basic models like linear and logistic regression.\n",
    "- **Advanced Machine Learning Algorithms:** A variety of sophisticated models such as decision trees, random forests, support vector machines, and clustering algorithms like k-means.\n",
    "- **Model Selection and Evaluation:** Methods for cross-validation, hyperparameter tuning, and model performance metrics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this lecture, we will:\n",
    "\n",
    "1. **Explore the Scikit-Learn Interface:** Understand the basic structure and design principles of `scikit-learn`.\n",
    "2. **Data Preprocessing:** Learn how to preprocess and prepare datasets for machine learning tasks.\n",
    "3. **Implement Basic Models:** Build and evaluate simple models such as linear and logistic regression.\n",
    "4. **Dive into Advanced Models:** Gain insights into more complex algorithms and their applications.\n",
    "5. **Model Evaluation and Tuning:** Learn techniques for assessing model performance and optimizing parameters.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Scikit-Learn**\n",
    "    - Overview and Installation\n",
    "    - Core Concepts and Design\n",
    "2. **Data Preprocessing**\n",
    "    - Loading Data and Splitting Data into Training and Testing Sets\n",
    "    - Handling Missing Values\n",
    "    - Feature Scaling and Normalization\n",
    "3. **Supervised Learning**\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Evaluation Metrics\n",
    "4. **Unsupervised Learning**\n",
    "    - Clustering Techniques\n",
    "    - Principal Component Analysis (PCA)\n",
    "5. **Model Selection and Tuning**\n",
    "    - Cross-Validation\n",
    "    - Grid Search and Random Search\n",
    "    - Performance Metrics\n",
    "\n",
    "Throughout this unit, we'll build a strong foundation in the fundamentals of `scikit-learn`, progressively moving towards more advanced and sophisticated applications in machine learning. By the end of this unit, you'll be equipped with the knowledge and skills to apply `scikit-learn` to a wide range of machine learning problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# we will import each individual scikit-learn separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cd0d6",
   "metadata": {},
   "source": [
    "# 1. Introduction to Scikit-Learn\n",
    "\n",
    "In this section, we will get an overview of Scikit-Learn, its installation, and core concepts. This foundational knowledge will help us understand how to effectively use the library for various machine learning tasks.\n",
    "\n",
    "## Overview and Installation\n",
    "\n",
    "### What is Scikit-Learn?\n",
    "\n",
    "`scikit-learn` is a powerful and flexible Python library for machine learning. It provides simple and efficient tools for data mining and data analysis, making it accessible for everyone. It is built on NumPy, SciPy, and Matplotlib.\n",
    "\n",
    "### Key Features of Scikit-Learn\n",
    "\n",
    "- **Easy-to-use API**: Simple and consistent interface for all models.\n",
    "- **Comprehensive Documentation**: Extensive and user-friendly documentation.\n",
    "- **Efficient Tools**: Optimized for performance and memory usage.\n",
    "- **Wide Range of Algorithms**: Includes many algorithms for classification, regression, clustering, and dimensionality reduction.\n",
    "\n",
    "### Installing Scikit-Learn\n",
    "\n",
    "To install `scikit-learn`, you can use `conda`:\n",
    "\n",
    "```bash\n",
    "conda install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89fbc1",
   "metadata": {},
   "source": [
    "## Core Concepts and Design\n",
    "\n",
    "### The Scikit-Learn API\n",
    "\n",
    "The Scikit-Learn API is designed with a few key principles in mind:\n",
    "\n",
    "1. **Consistency**: All objects share a consistent interface, making it easy to switch between models.\n",
    "2. **Inspection**: All hyperparameters are accessible directly via public attributes.\n",
    "3. **Composition**: Many tools can be combined together, like pipelines.\n",
    "4. **Non-proliferation of classes**: Rather than introducing a plethora of new classes, Scikit-Learn sticks to a few well-defined, task-specific objects.\n",
    "\n",
    "### Basic Objects in Scikit-Learn\n",
    "\n",
    "- **Estimators**: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., a classification algorithm). All estimators implement a `fit` method.\n",
    "- **Predictors**: An estimator that can also predict a value given an input (e.g., a classifier). All predictors implement a `predict` method.\n",
    "- **Transformers**: An estimator that can transform a dataset (e.g., a pre-processing step). All transformers implement a `transform` method.\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "Hereâ€™s a basic workflow to illustrate the use of Scikit-Learn:\n",
    "\n",
    "1. **Import the necessary modules**:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    ```\n",
    "\n",
    "2. **Load and split the data**:\n",
    "\n",
    "    ```python\n",
    "    iris = load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "    ```\n",
    "\n",
    "3. **Preprocess the data**:\n",
    "\n",
    "    ```python\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    ```\n",
    "\n",
    "4. **Train a model**:\n",
    "\n",
    "    ```python\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "5. **Make predictions and evaluate the model**:\n",
    "\n",
    "    ```python\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    ```\n",
    "\n",
    "This example highlights the simplicity and power of Scikit-Learn, showing how you can quickly build and evaluate a machine learning model.\n",
    "\n",
    "In the next sections, we will dive deeper into data preprocessing, supervised learning, unsupervised learning, and model selection and tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344654cc",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step in the machine learning pipeline. It involves preparing and cleaning the raw data to make it suitable for building machine learning models. In this section, we will cover various techniques and tools provided by Scikit-Learn to preprocess data effectively.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you will be able to:\n",
    "\n",
    "1. Load and split datasets using Scikit-Learn utilities.\n",
    "2. Handle missing values in your data.\n",
    "3. Scale and normalize features for better model performance.\n",
    "4. Encode categorical variables.\n",
    "\n",
    "## Loading and Splitting Data\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "Scikit-Learn provides several built-in datasets that are useful for practice and experimentation. You can load these datasets using the `datasets` module. We will use the famous Iris dataset as the example. \n",
    "\n",
    "The Iris dataset is one of the most well-known and widely used datasets in the field of machine learning and statistics. Introduced by the British biologist and statistician Ronald A. Fisher in 1936, the dataset consists of 150 observations of iris flowers from three different species: `Iris setosa`, `Iris versicolor`, and `Iris virginica`. Each observation includes four features: `sepal length`, `sepal width`, `petal length`, and `petal width`, measured in centimeters. \n",
    "\n",
    "The dataset is often used for demonstrating and testing various machine learning algorithms, as it provides a clear, easy-to-understand example of a multiclass classification problem. Its simplicity and well-defined structure make it an ideal starting point for beginners learning about data analysis and machine learning techniques.\n",
    "\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into features and target\n",
    "X, y = iris.data, iris.target \n",
    "print(type(X), X.shape)\n",
    "print(type(y), y.shape)\n",
    "\n",
    "###############################\n",
    "# Basic inspection of the data\n",
    "###############################\n",
    "\n",
    "# Create a DataFrame with the feature data\n",
    "df_features = pd.DataFrame(X, columns=iris.feature_names)\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "df_features['species'] = y\n",
    "\n",
    "# Map the target values to their corresponding class names\n",
    "df_features['species'] = df_features['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the Iris dataset:\")\n",
    "print(df_features.head())\n",
    "\n",
    "# Display the feature names\n",
    "print(\"\\nFeature names:\")\n",
    "print(iris.feature_names)\n",
    "\n",
    "# Display the target class names\n",
    "print(\"\\nTarget class names:\")\n",
    "print(iris.target_names)\n",
    "\n",
    "# Display basic statistics of the dataset\n",
    "print(\"\\nBasic statistics of the Iris dataset:\")\n",
    "print(df_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ec533",
   "metadata": {},
   "source": [
    "More details about this dataset can be found at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "\n",
    "A Full list of datasets provided by Scikit-learn can be found at https://scikit-learn.org/stable/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c6e00",
   "metadata": {},
   "source": [
    "In machine learning, especially `supervised learning` context, by convention, we use $X$ and $y$ to represent \n",
    "\n",
    "-   `features` <=> $X$ <=> `Independent variables`\n",
    "\n",
    "-   `target/Label` <=> $y$ <=> `Dependent variable`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe4842",
   "metadata": {},
   "source": [
    "Another common source of data we use is CSV (Comma-Separated Values) files. We've used these before, but let's review their structure and usage.\n",
    "\n",
    "In a CSV file, each row corresponds to a data point or observation, and each column refers to a variable or feature. The first row typically contains the header, which labels each column with the variable names. The subsequent rows contain the data values for each observation.\n",
    "\n",
    "CSV files are popular for data storage and exchange because they are simple to create and read. They are supported by many software applications, including spreadsheet programs like Microsoft Excel and Google Sheets, and are easily handled in Python using libraries such as `pandas`.\n",
    "\n",
    "Here is an example of a CSV file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52f746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product = pd.read_csv('data/product.csv',index_col=0)\n",
    "product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50680645",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- We use the `ProductID` column as the index column. \n",
    "- (`Price, Rating, Discount, Sales`) is the header row, defining the names of the columns.\n",
    "- Each subsequent row represents an individual observation with values for each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774c6d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this dataset, the dependent variable (or target) is `Sales`, which we want to predict. The independent variables (or features) are `Price`, `Rating`, and `Discount`. These features provide the information needed to make predictions about the `Sales`.\n",
    "\n",
    "When working with machine learning models, it's important to separate the target variable from the features. With the data loaded as it is, we need to manually split the columns to extract our feature matrix \\( X \\) and target vector \\( y \\). This process involves isolating the target variable (`Sales`) from the rest of the dataset.\n",
    "\n",
    "Here's how you can do it using `pandas`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'data/product.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X_product = df[['Price', 'Rating', 'Discount']]\n",
    "y_product = df['Sales']\n",
    "\n",
    "# Display the first few rows of X and y\n",
    "print(\"Features (X):\")\n",
    "print(X_product.head())\n",
    "print(\"\\nTarget (y):\")\n",
    "print(X_product.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3063a01",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets\n",
    "It is important to split your data into training and testing sets to evaluate your model's performance. Scikit-Learn provides the train_test_split function for this purpose.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c534d3",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Missing values can cause problems for machine learning models. Scikit-Learn provides the SimpleImputer class to handle missing values.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27776cb0",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization\n",
    "Feature scaling is essential to ensure that all features contribute equally to the model's performance. Scikit-Learn provides several transformers for scaling features, such as StandardScaler and MinMaxScaler.\n",
    "\n",
    "### Standard Scaling\n",
    "Standard scaling transforms the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaee7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb59038",
   "metadata": {},
   "source": [
    "### Min-Max Scaling\n",
    "Min-Max scaling transforms the data to a fixed range, usually 0 to 1.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef77019",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "Categorical variables need to be converted into numerical values for machine learning models. Scikit-Learn provides the OneHotEncoder and LabelEncoder classes for this purpose.\n",
    "\n",
    "### One-Hot Encoding\n",
    "One-hot encoding creates binary columns for each category.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this won't work for the iris dataset because there is no categorical columns in the iris features\n",
    "# If you run the code here, you will probably see error messages. \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab4ca8",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "Label encoding converts categories into integer labels.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this won't work for the iris dataset because there is no categorical columns in the iris features\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "y_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae97e6c",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Here is a complete example of data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load datab\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "numeric_features = [0, 1, 2, 3]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53d360",
   "metadata": {},
   "source": [
    "In the next section, we will explore supervised learning techniques, starting with linear and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80fa6b",
   "metadata": {},
   "source": [
    "# 3. Supervised Learning\n",
    "\n",
    "Supervised learning involves training a model on labeled data, which means that each training example is paired with an output label. The model learns to predict the output from the input data.\n",
    "\n",
    "## 3.1 Introduction to Supervised Learning\n",
    "\n",
    "Supervised learning algorithms are used for tasks where the goal is to predict a target variable. The main types of supervised learning problems are classification and regression.\n",
    "\n",
    "- **Classification:** Predict a discrete label.\n",
    "- **Regression:** Predict a continuous value.\n",
    "\n",
    "## 3.2 Linear Regression\n",
    "\n",
    "Linear regression is used for predicting a continuous value. It models the relationship between the dependent variable and one or more independent variables by fitting a linear equation to the observed data.\n",
    "\n",
    "\n",
    "Before moving onto code, let's first try to understand what Linear Regression is.\n",
    "\n",
    "- Watch [Linear Regression Video 1](https://www.youtube.com/watch?v=CtsRRUddV2s)\n",
    "- Watch [Linear Regression Video 2](https://www.youtube.com/watch?v=PaFPbb66DxQ)\n",
    "\n",
    "\n",
    "\n",
    "### Example of Linear Regression\n",
    "\n",
    "- **Data Generation**: We create synthetic data using the equation \\( y = 4 + 3X + NOISE \\), where noise is a random value to simulate real-world data.\n",
    "- **Data Splitting**: We split the data into training and testing sets using `train_test_split`.\n",
    "- **Model Training**: We create a `LinearRegression` model and train it on the training data.\n",
    "- **Prediction**: We use the trained model to make predictions on the test data.\n",
    "- **Plotting**: We plot the original data points and the regression line to visualize the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84140477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model:\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Extract the coefficients and intercept\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737bd87",
   "metadata": {},
   "source": [
    "#### Save a model\n",
    "\n",
    "If you want to save the model for future reuse, you can use the `joblib` library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8040f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save the model to a file\n",
    "dump(model, 'linear_regression_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b0a39",
   "metadata": {},
   "source": [
    "This will save your model object to a file named 'linear_regression_model.joblib' in the current working directory. Later, you can load the model back into your code using load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "loaded_model = load('linear_regression_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6c5bf",
   "metadata": {},
   "source": [
    "Now, loaded_model is a new LinearRegression object that is a copy of the model you saved.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68657121",
   "metadata": {},
   "source": [
    "## 3.3 Logistic Regression\n",
    "Logistic regression is used for binary classification problems. It models the probability that a given input point belongs to a certain class. \n",
    "\n",
    "[Logistic Regression StatQuest Video Playlist](https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&index=1)\n",
    "### Example of Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c99aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2553a5",
   "metadata": {},
   "source": [
    "## 3.4 Evaluation Metrics\n",
    "Evaluation metrics are used to assess the performance of a machine learning model. For regression and classification tasks, different metrics are used.\n",
    "\n",
    "### Regression Metrics\n",
    "- Mean Squared Error (MSE): Measures the average of the squares of the errors.\n",
    "- Root Mean Squared Error (RMSE): The square root of the average of squared differences between prediction and actual observation.\n",
    "- Mean Absolute Error (MAE): Measures the average of the absolute errors.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06773c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12b84c",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "- Accuracy: The ratio of correctly predicted instances to the total instances.\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "- Recall: The ratio of correctly predicted positive observations to all observations in the actual class.\n",
    "- F1 Score: The weighted average of Precision and Recall.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9b566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate Precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Calculate Recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9efe4",
   "metadata": {},
   "source": [
    "In the next sections, we will delve deeper into other supervised learning techniques and their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57125b7",
   "metadata": {},
   "source": [
    "# 4. Unsupervised Learning\n",
    "\n",
    "Unsupervised learning involves training a model on data that has no labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data.\n",
    "\n",
    "## 4.1 Introduction to Unsupervised Learning\n",
    "\n",
    "Unsupervised learning algorithms are used for tasks where the data is not labeled. The main types of unsupervised learning problems are clustering and dimensionality reduction.\n",
    "\n",
    "- **Clustering:** Grouping similar data points together.\n",
    "- **Dimensionality Reduction:** Reducing the number of features in the data while retaining its essential information.\n",
    "\n",
    "## 4.2 Clustering Techniques\n",
    "\n",
    "Clustering is a technique used to group similar data points together based on their features. One of the most common clustering algorithms is k-means clustering.\n",
    "\n",
    "### K-Means Clustering\n",
    "\n",
    "K-means clustering aims to partition the data into k clusters, where each data point belongs to the cluster with the nearest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a18c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example of K-Means Clustering\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d882cd5",
   "metadata": {},
   "source": [
    "## 4.3 Principal Component Analysis (PCA)\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a new coordinate system. The greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "### Example of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the PCA-transformed data\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=iris.target_names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874411a",
   "metadata": {},
   "source": [
    "In the next sections, we will delve deeper into model selection and tuning, and explore advanced machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c1f33",
   "metadata": {},
   "source": [
    "# 5. Model Selection and Tuning\n",
    "\n",
    "Model selection and tuning are crucial steps in the machine learning pipeline to ensure that the chosen model performs well on unseen data. In this section, we will cover techniques for selecting the right model, tuning its hyperparameters, and evaluating its performance.\n",
    "\n",
    "## 5.1 Cross-Validation\n",
    "\n",
    "Cross-validation is a technique used to assess how well a model will generalize to an independent dataset. It involves splitting the data into multiple folds, training the model on different subsets, and evaluating its performance on the remaining data.\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "K-Fold Cross-Validation splits the data into k folds, trains the model on k-1 folds, and evaluates it on the remaining fold. This process is repeated k times, with each fold used once as the validation data.\n",
    "\n",
    "#### Example of K-Fold Cross-Validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Accuracy: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c9105",
   "metadata": {},
   "source": [
    "## 5.2 Grid Search and Random Search\n",
    "Grid Search and Random Search are techniques used to find the best hyperparameters for a model.\n",
    "\n",
    "### Grid Search\n",
    "Grid Search is a technique that searches for the optimal hyperparameters by evaluating the model performance for each combination of hyperparameters in a predefined grid.\n",
    "\n",
    "Example of Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af596f9",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Random Search is a technique that searches for the optimal hyperparameters by selecting random combinations of hyperparameters and evaluating their performance.\n",
    "\n",
    "Example of Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define hyperparameters distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "# Create a model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Perform Random Search\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=5, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5949e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
